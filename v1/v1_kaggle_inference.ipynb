{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CSIRO Image2Biomass v1 Kaggle Inference\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 설정 및 경로\n",
        "\n",
        "import glob\n",
        "import os\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "sys.path.append(os.path.abspath(\".\"))\n",
        "\n",
        "from src.config import Config, PathConfig, TrainConfig, OptunaConfig\n",
        "from src.data import RegressionDataset, load_long_dataframe, to_wide\n",
        "from src.metrics import expand_targets\n",
        "from src.model import build_model\n",
        "\n",
        "\n",
        "def ensure_timm_installed() -> None:\n",
        "    try:\n",
        "        import timm  # noqa: F401\n",
        "    except ImportError as exc:\n",
        "        raise ImportError(\n",
        "            \"timm이 설치되어 있어야 합니다. Kaggle 환경에 timm 패키지가 포함된 런타임/데이터셋을 추가해주세요.\"\n",
        "        ) from exc\n",
        "\n",
        "\n",
        "DATA_ROOT = \"/kaggle/input/csiro-biomass\"\n",
        "WEIGHTS_ROOT = \"/kaggle/input/csiroi2b-weights\"  # Kaggle Dataset 경로에 맞게 수정하세요.\n",
        "RUN_NAME = os.environ.get(\"RUN_NAME\", \"v1_inference\")\n",
        "OUTPUT_ROOT = \"/kaggle/working/outputs\"\n",
        "RUN_DIR = os.path.join(OUTPUT_ROOT, RUN_NAME)\n",
        "SUBMISSION_PATH = os.path.join(RUN_DIR, \"submission\", \"submission.csv\")\n",
        "WORKING_SUBMISSION = \"/kaggle/working/submission.csv\"\n",
        "\n",
        "os.makedirs(os.path.join(RUN_DIR, \"submission\"), exist_ok=True)\n",
        "\n",
        "cfg = Config(\n",
        "    paths=PathConfig(\n",
        "        data_root=DATA_ROOT,\n",
        "        train_csv=\"train.csv\",\n",
        "        test_csv=\"test.csv\",\n",
        "        output_root=OUTPUT_ROOT,\n",
        "        run_name=RUN_NAME,\n",
        "    ),\n",
        "    train=TrainConfig(),\n",
        "    optuna=OptunaConfig(use_optuna=False),\n",
        "    device=\"cuda\",\n",
        ")\n",
        "\n",
        "device = torch.device(cfg.device if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 데이터 확인 및 로딩\n",
        "\n",
        "ensure_timm_installed()\n",
        "\n",
        "train_csv = cfg.paths.resolve_train_csv()\n",
        "test_csv = cfg.paths.resolve_test_csv()\n",
        "\n",
        "print(\"Train CSV exists:\", os.path.exists(train_csv), \"-\", train_csv)\n",
        "print(\"Test CSV exists:\", os.path.exists(test_csv), \"-\", test_csv)\n",
        "\n",
        "train_long = load_long_dataframe(train_csv)\n",
        "test_long = load_long_dataframe(test_csv)\n",
        "\n",
        "print(f\"Train rows: {len(train_long)}, columns: {train_long.columns.tolist()}\")\n",
        "print(f\"Test rows: {len(test_long)}, columns: {test_long.columns.tolist()}\")\n",
        "\n",
        "train_images = glob.glob(os.path.join(DATA_ROOT, \"train\", \"*.jpg\"))\n",
        "test_images = glob.glob(os.path.join(DATA_ROOT, \"test\", \"*.jpg\"))\n",
        "print(f\"Detected train images: {len(train_images)}\")\n",
        "print(f\"Detected test images: {len(test_images)}\")\n",
        "\n",
        "test_wide = to_wide(test_long, include_targets=False)\n",
        "print(\"test_wide shape:\", test_wide.shape)\n",
        "test_wide.head()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataset 및 Transform (추론 전용 DataLoader)\n",
        "\n",
        "\n",
        "def get_inference_loader(test_df: pd.DataFrame) -> DataLoader:\n",
        "    ds = RegressionDataset(\n",
        "        test_df,\n",
        "        cfg.paths.resolve_image_root(),\n",
        "        cfg.train.image_size,\n",
        "        augment=False,\n",
        "        use_targets=False,\n",
        "    )\n",
        "    return DataLoader(\n",
        "        ds,\n",
        "        batch_size=cfg.train.batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=cfg.train.num_workers,\n",
        "        pin_memory=True,\n",
        "    )\n",
        "\n",
        "\n",
        "inference_loader = get_inference_loader(test_wide)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 모델 정의 (pretrained=False, 학습된 state_dict 로드 예정)\n",
        "\n",
        "\n",
        "def load_model(checkpoint_path: str) -> torch.nn.Module:\n",
        "    model = build_model(cfg.train.backbone, pretrained=False)\n",
        "    state = torch.load(checkpoint_path, map_location=device)\n",
        "    model.load_state_dict(state)\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    return model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 추론 및 제출 파일 생성\n",
        "\n",
        "\n",
        "def predict_wide(loader: DataLoader) -> np.ndarray:\n",
        "    ckpts = sorted(glob.glob(os.path.join(WEIGHTS_ROOT, \"fold*_best.pth\")))\n",
        "    if not ckpts:\n",
        "        raise FileNotFoundError(f\"No checkpoint files found in {WEIGHTS_ROOT}.\")\n",
        "\n",
        "    preds_stack = []\n",
        "    for ckpt_path in ckpts:\n",
        "        model = load_model(ckpt_path)\n",
        "        fold_preds = []\n",
        "        with torch.no_grad():\n",
        "            for images, _, _ in tqdm(loader, desc=f\"Predict {os.path.basename(ckpt_path)}\"):\n",
        "                images = images.to(device)\n",
        "                outputs = model(images)\n",
        "                fold_preds.append(outputs.cpu().numpy())\n",
        "        preds_stack.append(np.concatenate(fold_preds))\n",
        "\n",
        "    preds = np.mean(preds_stack, axis=0)\n",
        "    return preds\n",
        "\n",
        "\n",
        "def build_submission(test_long_df: pd.DataFrame, test_wide_df: pd.DataFrame, preds: np.ndarray, run_dir: str) -> str:\n",
        "    full_preds = expand_targets(preds)\n",
        "    pred_df = pd.DataFrame(full_preds, columns=[\"Dry_Green_g\", \"Dry_Dead_g\", \"Dry_Clover_g\", \"GDM_g\", \"Dry_Total_g\"])\n",
        "    pred_df[\"sample_id_prefix\"] = test_wide_df[\"sample_id_prefix\"].values\n",
        "\n",
        "    pred_long = pred_df.melt(id_vars=\"sample_id_prefix\", var_name=\"target_name\", value_name=\"target\")\n",
        "    pred_long[\"sample_id\"] = pred_long[\"sample_id_prefix\"].astype(str) + \"__\" + pred_long[\"target_name\"].astype(str)\n",
        "\n",
        "    merged = test_long_df.merge(\n",
        "        pred_long[[\"sample_id_prefix\", \"target_name\", \"target\"]],\n",
        "        on=[\"sample_id_prefix\", \"target_name\"],\n",
        "        how=\"left\",\n",
        "    )\n",
        "\n",
        "    submission = merged[[\"sample_id\", \"target\"]].copy()\n",
        "    os.makedirs(os.path.join(run_dir, \"submission\"), exist_ok=True)\n",
        "    submission.to_csv(SUBMISSION_PATH, index=False)\n",
        "    return SUBMISSION_PATH\n",
        "\n",
        "\n",
        "def run_inference_and_save(test_long_df: pd.DataFrame, test_wide_df: pd.DataFrame) -> str:\n",
        "    loader = get_inference_loader(test_wide_df)\n",
        "    preds = predict_wide(loader)\n",
        "    submission_path = build_submission(test_long_df, test_wide_df, preds, RUN_DIR)\n",
        "\n",
        "    os.makedirs(os.path.dirname(WORKING_SUBMISSION), exist_ok=True)\n",
        "    pd.read_csv(submission_path).to_csv(WORKING_SUBMISSION, index=False)\n",
        "    print(\"Saved submission to:\", submission_path)\n",
        "    print(\"Copied submission to:\", WORKING_SUBMISSION)\n",
        "    return submission_path\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 제출 파일 검증 및 출력\n",
        "\n",
        "submission_path = run_inference_and_save(test_long, test_wide)\n",
        "\n",
        "submission = pd.read_csv(submission_path)\n",
        "print(\"Submission shape:\", submission.shape)\n",
        "print(\"Submission columns:\", submission.columns.tolist())\n",
        "print(\"NaN present:\", submission[\"target\"].isna().any())\n",
        "\n",
        "sample_submission_path = os.path.join(DATA_ROOT, \"sample_submission.csv\")\n",
        "if os.path.exists(sample_submission_path):\n",
        "    sample_sub = pd.read_csv(sample_submission_path)\n",
        "    sample_ids = set(sample_sub[\"sample_id\"])\n",
        "    submission_ids = set(submission[\"sample_id\"])\n",
        "    missing_ids = sample_ids - submission_ids\n",
        "    extra_ids = submission_ids - sample_ids\n",
        "    print(\"Missing IDs compared to sample_submission:\", len(missing_ids))\n",
        "    print(\"Extra IDs compared to sample_submission:\", len(extra_ids))\n",
        "else:\n",
        "    print(\"sample_submission.csv not found at\", sample_submission_path)\n",
        "\n",
        "print(\"Final submission path:\", submission_path)\n",
        "print(\"Working copy path:\", WORKING_SUBMISSION)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}