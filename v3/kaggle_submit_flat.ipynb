{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CSIRO Image2Biomass v3 \u2014 Flattened Kaggle Submission (Inference Only)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Cell 1: imports + paths/config (edit WEIGHTS_ROOT)\n",
        "import glob\n",
        "import os\n",
        "from typing import List\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import timm\n",
        "import torch\n",
        "from PIL import Image, ImageOps\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as T\n",
        "from tqdm import tqdm\n",
        "\n",
        "try:\n",
        "    import cv2\n",
        "except ImportError:\n",
        "    cv2 = None\n",
        "\n",
        "# === Mandatory path settings for offline Kaggle inference ===\n",
        "DATA_ROOT = \"/kaggle/input/csiro-biomass\"\n",
        "# !!! Update this to point to the attached Kaggle Dataset that contains fold*_best.pth files\n",
        "WEIGHTS_ROOT = \"/kaggle/input/<your-weights-dataset>/v3_weights\"\n",
        "\n",
        "RUN_NAME = os.environ.get(\"RUN_NAME\", \"v3_flat_inference\")\n",
        "OUTPUT_ROOT = \"/kaggle/working/outputs\"\n",
        "RUN_DIR = os.path.join(OUTPUT_ROOT, RUN_NAME)\n",
        "SUBMISSION_PATH = os.path.join(RUN_DIR, \"submission\", \"submission.csv\")\n",
        "WORKING_SUBMISSION = \"/kaggle/working/submission.csv\"\n",
        "\n",
        "# Preprocessing + split metadata for reproducibility\n",
        "CROP_BOTTOM = 0.1  # crop away bottom 10% of image height; set to 0.0 to disable\n",
        "USE_CLAHE = True   # contrast normalization; falls back to PIL equalize if OpenCV is unavailable\n",
        "CV_SPLIT_STRATEGY = \"group_date_state\"  # training-time CV grouping; shown here for reference\n",
        "\n",
        "os.makedirs(os.path.join(RUN_DIR, \"submission\"), exist_ok=True)\n",
        "\n",
        "TARGET_COLUMNS = [\"Dry_Green_g\", \"Dry_Clover_g\", \"Dry_Dead_g\"]\n",
        "ALL_TARGET_COLUMNS = TARGET_COLUMNS + [\"GDM_g\", \"Dry_Total_g\"]\n",
        "\n",
        "cfg = {\n",
        "    \"backbone\": \"efficientnet_b2\",  # same as v3/src config\n",
        "    \"image_size\": 456,\n",
        "    \"batch_size\": 32,\n",
        "    \"num_workers\": 2,\n",
        "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "}\n",
        "\n",
        "device = torch.device(cfg[\"device\"])\n",
        "print(f\"Using device: {device}\")\n",
        "print(f\"DATA_ROOT: {DATA_ROOT}\")\n",
        "print(f\"WEIGHTS_ROOT: {WEIGHTS_ROOT}\")\n",
        "print(\"cv2 available:\", cv2 is not None)\n",
        "\n",
        "\n",
        "def expand_targets(primary: np.ndarray) -> np.ndarray:\n",
        "    dry_green = primary[:, 0]\n",
        "    dry_clover = primary[:, 1]\n",
        "    dry_dead = primary[:, 2]\n",
        "    gdm = dry_green + dry_clover\n",
        "    dry_total = gdm + dry_dead\n",
        "    full = np.stack([dry_green, dry_dead, dry_clover, gdm, dry_total], axis=1)\n",
        "    return full\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Cell 2: quick file/column sanity check\n",
        "\n",
        "def load_long_dataframe(csv_path: str) -> pd.DataFrame:\n",
        "    df = pd.read_csv(csv_path)\n",
        "    required = {\"sample_id\", \"image_path\"}\n",
        "    missing = required - set(df.columns)\n",
        "    if missing:\n",
        "        raise ValueError(f\"Missing columns in {csv_path}: {sorted(missing)}\")\n",
        "    df[\"sample_id_prefix\"] = df[\"sample_id\"].str.split(\"__\").str[0]\n",
        "    return df\n",
        "\n",
        "\n",
        "def to_wide(df: pd.DataFrame, include_targets: bool = True) -> pd.DataFrame:\n",
        "    index_cols = [col for col in [\"sample_id_prefix\", \"image_path\"] if col in df.columns]\n",
        "    missing_index = [col for col in [\"sample_id_prefix\", \"image_path\"] if col not in index_cols]\n",
        "    if missing_index:\n",
        "        raise ValueError(f\"Missing required aggregation columns: {missing_index}\")\n",
        "\n",
        "    if not include_targets:\n",
        "        return df[index_cols].drop_duplicates().reset_index(drop=True)\n",
        "\n",
        "    if \"target_name\" not in df.columns or \"target\" not in df.columns:\n",
        "        raise ValueError(\"target_name/target columns are required when include_targets=True\")\n",
        "\n",
        "    wide = df.pivot_table(index=index_cols, columns=\"target_name\", values=\"target\", aggfunc=\"first\").reset_index()\n",
        "    missing = [c for c in TARGET_COLUMNS if c not in wide.columns]\n",
        "    if missing:\n",
        "        raise ValueError(f\"Missing target columns after pivot: {missing}\")\n",
        "    return wide\n",
        "\n",
        "\n",
        "test_csv = os.path.join(DATA_ROOT, \"test.csv\")\n",
        "sample_submission_csv = os.path.join(DATA_ROOT, \"sample_submission.csv\")\n",
        "\n",
        "print(\"Test CSV exists:\", os.path.exists(test_csv), \"-\", test_csv)\n",
        "print(\"sample_submission.csv exists:\", os.path.exists(sample_submission_csv), \"-\", sample_submission_csv)\n",
        "\n",
        "if not os.path.exists(test_csv):\n",
        "    raise FileNotFoundError(f\"test.csv not found at {test_csv}\")\n",
        "\n",
        "# Load test set and derive wide format (no targets)\n",
        "test_long = load_long_dataframe(test_csv)\n",
        "print(f\"Test rows: {len(test_long)}, columns: {test_long.columns.tolist()}\")\n",
        "\n",
        "test_wide = to_wide(test_long, include_targets=False)\n",
        "print(\"test_wide shape:\", test_wide.shape)\n",
        "print(test_wide.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Cell 3: dataset & transforms (inference-only)\n",
        "\n",
        "class RegressionDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, df: pd.DataFrame, image_root: str, image_size: int, crop_bottom: float = 0.0, use_clahe: bool = False):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.image_root = image_root\n",
        "        self.crop_bottom = crop_bottom\n",
        "        self.use_clahe = use_clahe\n",
        "        self.transform = T.Compose([\n",
        "            T.Resize((image_size, image_size)),\n",
        "            T.ToTensor(),\n",
        "            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        row = self.df.iloc[idx]\n",
        "        img_path = os.path.normpath(os.path.join(self.image_root, row[\"image_path\"]))\n",
        "        with Image.open(img_path) as img:\n",
        "            image = img.convert(\"RGB\")\n",
        "\n",
        "        if self.crop_bottom > 0:\n",
        "            width, height = image.size\n",
        "            keep_height = max(1, int(round(height * (1 - self.crop_bottom))))\n",
        "            image = image.crop((0, 0, width, keep_height))\n",
        "\n",
        "        if self.use_clahe:\n",
        "            image = self._apply_clahe(image)\n",
        "\n",
        "        image = self.transform(image)\n",
        "        return image, torch.zeros(len(TARGET_COLUMNS), dtype=torch.float32), row[\"sample_id_prefix\"]\n",
        "\n",
        "    def _apply_clahe(self, image: Image.Image) -> Image.Image:\n",
        "        if cv2 is None:\n",
        "            return ImageOps.equalize(image)\n",
        "\n",
        "        arr = np.array(image)\n",
        "        lab = cv2.cvtColor(arr, cv2.COLOR_RGB2LAB)\n",
        "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
        "        l, a, b = cv2.split(lab)\n",
        "        l_eq = clahe.apply(l)\n",
        "        lab_eq = cv2.merge((l_eq, a, b))\n",
        "        rgb = cv2.cvtColor(lab_eq, cv2.COLOR_LAB2RGB)\n",
        "        return Image.fromarray(rgb)\n",
        "\n",
        "\n",
        "def get_inference_loader(test_df: pd.DataFrame) -> DataLoader:\n",
        "    ds = RegressionDataset(test_df, DATA_ROOT, cfg[\"image_size\"], crop_bottom=CROP_BOTTOM, use_clahe=USE_CLAHE)\n",
        "    return DataLoader(\n",
        "        ds,\n",
        "        batch_size=cfg[\"batch_size\"],\n",
        "        shuffle=False,\n",
        "        num_workers=cfg[\"num_workers\"],\n",
        "        pin_memory=True,\n",
        "    )\n",
        "\n",
        "\n",
        "inference_loader = get_inference_loader(test_wide)\n",
        "print(\"Inference batches:\", len(inference_loader))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Cell 4: model definition (pretrained=False, weights loaded from dataset)\n",
        "\n",
        "def build_model(backbone: str, num_outputs: int = len(TARGET_COLUMNS)) -> torch.nn.Module:\n",
        "    # pretrained=False to avoid internet fetch in offline Kaggle\n",
        "    model = timm.create_model(backbone, pretrained=False, num_classes=num_outputs)\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Cell 5: checkpoint load + inference across folds (mean ensemble)\n",
        "\n",
        "def list_checkpoints(weights_root: str) -> List[str]:\n",
        "    ckpts = sorted(glob.glob(os.path.join(weights_root, \"fold*_best.pth\")))\n",
        "    if not ckpts:\n",
        "        raise FileNotFoundError(f\"No *_best.pth checkpoints found under {weights_root}\")\n",
        "    if len(ckpts) != 5:\n",
        "        print(f\"Warning: expected 5 fold checkpoints, found {len(ckpts)}\")\n",
        "    return ckpts\n",
        "\n",
        "\n",
        "def load_model(checkpoint_path: str) -> torch.nn.Module:\n",
        "    model = build_model(cfg[\"backbone\"], num_outputs=len(TARGET_COLUMNS))\n",
        "    state = torch.load(checkpoint_path, map_location=device)\n",
        "    model.load_state_dict(state)\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "\n",
        "def predict_wide(loader: DataLoader) -> np.ndarray:\n",
        "    checkpoints = list_checkpoints(WEIGHTS_ROOT)\n",
        "    preds_stack: List[np.ndarray] = []\n",
        "\n",
        "    for ckpt_path in checkpoints:\n",
        "        model = load_model(ckpt_path)\n",
        "        fold_preds = []\n",
        "        with torch.no_grad():\n",
        "            for images, _, _ in tqdm(loader, desc=f\"Predict {os.path.basename(ckpt_path)}\"):\n",
        "                images = images.to(device)\n",
        "                outputs = model(images)\n",
        "                fold_preds.append(outputs.cpu().numpy())\n",
        "        preds_stack.append(np.concatenate(fold_preds))\n",
        "\n",
        "    preds_mean = np.mean(preds_stack, axis=0)\n",
        "    print(\"Preds shape (mean ensemble):\", preds_mean.shape)\n",
        "    return preds_mean\n",
        "\n",
        "\n",
        "preds_primary = predict_wide(inference_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Cell 6: submission build + validation + save\n",
        "\n",
        "def build_submission(test_long_df: pd.DataFrame, test_wide_df: pd.DataFrame, preds: np.ndarray, run_dir: str) -> str:\n",
        "    full_preds = expand_targets(preds)\n",
        "    pred_df = pd.DataFrame(full_preds, columns=[\"Dry_Green_g\", \"Dry_Dead_g\", \"Dry_Clover_g\", \"GDM_g\", \"Dry_Total_g\"])\n",
        "    pred_df[\"sample_id_prefix\"] = test_wide_df[\"sample_id_prefix\"].values\n",
        "\n",
        "    pred_long = pred_df.melt(id_vars=\"sample_id_prefix\", var_name=\"target_name\", value_name=\"target\")\n",
        "    pred_long[\"sample_id\"] = pred_long[\"sample_id_prefix\"].astype(str) + \"__\" + pred_long[\"target_name\"].astype(str)\n",
        "\n",
        "    merged = test_long_df.merge(\n",
        "        pred_long[[\"sample_id_prefix\", \"target_name\", \"target\"]],\n",
        "        on=[\"sample_id_prefix\", \"target_name\"],\n",
        "        how=\"left\",\n",
        "    )\n",
        "\n",
        "    submission = merged[[\"sample_id\", \"target\"]].copy()\n",
        "    os.makedirs(os.path.join(run_dir, \"submission\"), exist_ok=True)\n",
        "    submission.to_csv(SUBMISSION_PATH, index=False)\n",
        "    submission.to_csv(WORKING_SUBMISSION, index=False)\n",
        "    return submission\n",
        "\n",
        "\n",
        "submission_df = build_submission(test_long, test_wide, preds_primary, RUN_DIR)\n",
        "print(\"Submission shape:\", submission_df.shape)\n",
        "print(\"NaN present:\", submission_df[\"target\"].isna().any())\n",
        "\n",
        "if os.path.exists(sample_submission_csv):\n",
        "    sample_sub = pd.read_csv(sample_submission_csv)\n",
        "    sample_ids = sample_sub[\"sample_id\"].tolist()\n",
        "    submission_ids = submission_df[\"sample_id\"].tolist()\n",
        "    print(\"Matches sample_submission length:\", len(sample_ids) == len(submission_ids))\n",
        "    print(\"Sample ID order identical:\", sample_ids == submission_ids)\n",
        "else:\n",
        "    print(\"sample_submission.csv not found at\", sample_submission_csv)\n",
        "\n",
        "print(\"Saved submission to:\", SUBMISSION_PATH)\n",
        "print(\"Copied submission to:\", WORKING_SUBMISSION)\n",
        "submission_df.head()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
