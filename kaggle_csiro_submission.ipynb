{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Kaggle CSIRO Biomass Submission\n",
        "\n",
        "Self-contained inference notebook that mirrors the repository's two-stream DINO logic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================================\n",
        "# USER-EDITABLE CONFIG\n",
        "# (Edit these paths only)\n",
        "# ================================\n",
        "TEST_CSV_PATH = \"/kaggle/input/csiro-biomass/test.csv\"\n",
        "TEST_IMAGE_DIR = \"/kaggle/input/csiro-biomass/test\"\n",
        "\n",
        "# List of checkpoint paths (Kaggle dataset paths)\n",
        "CHECKPOINTS = [\n",
        "    # e.g. \"/kaggle/input/my-csiro-weights/fold1.pth\",\n",
        "    #      \"/kaggle/input/my-csiro-weights/fold2.pth\",\n",
        "    #      ...\n",
        "]\n",
        "\n",
        "# (Optional) if I use multiple ensembles:\n",
        "ENSEMBLE_GROUPS = [\n",
        "    {\n",
        "        \"name\": \"ens1\",\n",
        "        \"weights\": [1.0],  # list aligned with CHECKPOINTS subset\n",
        "        \"ckpts\": [\n",
        "            # \"/kaggle/input/my-csiro-weights/fold1.pth\",\n",
        "            # \"/kaggle/input/my-csiro-weights/fold2.pth\",\n",
        "        ],\n",
        "    },\n",
        "    # I can add more groups later if needed\n",
        "]\n",
        "\n",
        "SUBMISSION_FILE = \"submission.csv\"\n",
        "BATCH_SIZE = 1\n",
        "NUM_WORKERS = 0\n",
        "\n",
        "# Grid used during training for tiled models (e.g. (2, 2))\n",
        "GRID = (2, 2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: install packages if missing on local runs (Kaggle images already include these)\n",
        "# !pip install -q timm albumentations\n",
        "\n",
        "import os, gc, math, types\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple, Dict, Any\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import timm\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", DEVICE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataset utilities --------------------------------------------------\n",
        "class TestBiomassDataset(Dataset):\n",
        "    def __init__(self, df: pd.DataFrame, image_dir: str, transform: A.BasicTransform, input_res: int) -> None:\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "        self.input_res = input_res\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        row = self.df.iloc[idx]\n",
        "        img_path = Path(self.image_dir) / row[\"image_path\"]\n",
        "        image = cv2.imread(str(img_path))\n",
        "        if image is None:\n",
        "            raise FileNotFoundError(f\"Image not found: {img_path}\")\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Split into left/right halves\n",
        "        h, w, _ = image.shape\n",
        "        mid = w // 2\n",
        "        left = image[:, :mid]\n",
        "        right = image[:, mid:]\n",
        "\n",
        "        left = self.transform(image=left)[\"image\"]\n",
        "        right = self.transform(image=right)[\"image\"]\n",
        "        return left, right\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model & backbone builder -------------------------------------------\n",
        "def _infer_input_res(backbone_name: str) -> int:\n",
        "    try:\n",
        "        cfg = timm.create_model(backbone_name, pretrained=False).default_cfg\n",
        "        size = cfg.get(\"input_size\", None)\n",
        "        if size is not None and len(size) == 3:\n",
        "            return int(size[1])\n",
        "    except Exception:\n",
        "        pass\n",
        "    return 224\n",
        "\n",
        "\n",
        "def _build_dino_by_name(backbone_name: str, input_res: int):\n",
        "    model = timm.create_model(\n",
        "        backbone_name,\n",
        "        pretrained=False,\n",
        "        num_classes=0,\n",
        "        global_pool=\"avg\",\n",
        "    )\n",
        "    if hasattr(model, \"patch_embed\") and hasattr(model.patch_embed, \"img_size\"):\n",
        "        try:\n",
        "            model.patch_embed.img_size = (input_res, input_res)\n",
        "        except Exception:\n",
        "            pass\n",
        "    return model\n",
        "\n",
        "\n",
        "class FiLM(nn.Module):\n",
        "    def __init__(self, in_dim: int, hidden: int = 512) -> None:\n",
        "        super().__init__()\n",
        "        self.gamma = nn.Sequential(\n",
        "            nn.Linear(in_dim, hidden),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(hidden, in_dim),\n",
        "        )\n",
        "        self.beta = nn.Sequential(\n",
        "            nn.Linear(in_dim, hidden),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(hidden, in_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return x * (1 + self.gamma(x)) + self.beta(x)\n",
        "\n",
        "\n",
        "class TwoStreamDINOBase(nn.Module):\n",
        "    def __init__(self, backbone_name: str, input_res: int, grid: Tuple[int, int] = (1, 1)) -> None:\n",
        "        super().__init__()\n",
        "        self.backbone_name = backbone_name\n",
        "        self.backbone = _build_dino_by_name(backbone_name, input_res)\n",
        "        self.grid = grid\n",
        "        self.feature_dim = getattr(self.backbone, \"num_features\", 1024)\n",
        "        hidden = self.feature_dim * 2\n",
        "        self.head_total = nn.Sequential(nn.Linear(hidden, hidden), nn.GELU(), nn.Linear(hidden, 1))\n",
        "        self.head_gdm = nn.Sequential(nn.Linear(hidden, hidden), nn.GELU(), nn.Linear(hidden, 1))\n",
        "        self.head_green = nn.Sequential(nn.Linear(hidden, hidden), nn.GELU(), nn.Linear(hidden, 1))\n",
        "        self.softplus = nn.Softplus()\n",
        "\n",
        "    def _encode_backbone(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        feats = self.backbone(x)\n",
        "        if isinstance(feats, (list, tuple)):\n",
        "            feats = feats[-1]\n",
        "        return feats\n",
        "\n",
        "    def _tile_image(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # x: (B, C, H, W)\n",
        "        rows, cols = self.grid\n",
        "        b, c, h, w = x.shape\n",
        "        h_step, w_step = h // rows, w // cols\n",
        "        tiles = []\n",
        "        for r in range(rows):\n",
        "            for cidx in range(cols):\n",
        "                tiles.append(x[:, :, r * h_step : (r + 1) * h_step, cidx * w_step : (cidx + 1) * w_step])\n",
        "        return torch.stack(tiles, dim=1)  # (B, T, C, h_step, w_step)\n",
        "\n",
        "    def forward(self, x_left: torch.Tensor, x_right: torch.Tensor):\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "class TwoStreamDINOPlain(TwoStreamDINOBase):\n",
        "    def forward(self, x_left: torch.Tensor, x_right: torch.Tensor):\n",
        "        f_left = self._encode_backbone(x_left)\n",
        "        f_right = self._encode_backbone(x_right)\n",
        "        fused = torch.cat([f_left, f_right], dim=1)\n",
        "        total = self.softplus(self.head_total(fused))\n",
        "        gdm = self.softplus(self.head_gdm(fused))\n",
        "        green = self.softplus(self.head_green(fused))\n",
        "        return total, gdm, green\n",
        "\n",
        "\n",
        "class TwoStreamDINOTiled(TwoStreamDINOBase):\n",
        "    def forward(self, x_left: torch.Tensor, x_right: torch.Tensor):\n",
        "        left_tiles = self._tile_image(x_left)\n",
        "        right_tiles = self._tile_image(x_right)\n",
        "        b, t, c, h, w = left_tiles.shape\n",
        "        left_tiles = left_tiles.view(b * t, c, h, w)\n",
        "        right_tiles = right_tiles.view(b * t, c, h, w)\n",
        "        f_left = self._encode_backbone(left_tiles).view(b, t, -1).mean(dim=1)\n",
        "        f_right = self._encode_backbone(right_tiles).view(b, t, -1).mean(dim=1)\n",
        "        fused = torch.cat([f_left, f_right], dim=1)\n",
        "        total = self.softplus(self.head_total(fused))\n",
        "        gdm = self.softplus(self.head_gdm(fused))\n",
        "        green = self.softplus(self.head_green(fused))\n",
        "        return total, gdm, green\n",
        "\n",
        "\n",
        "class TwoStreamDINOTiledFiLM(TwoStreamDINOBase):\n",
        "    def __init__(self, backbone_name: str, input_res: int, grid: Tuple[int, int]) -> None:\n",
        "        super().__init__(backbone_name, input_res, grid)\n",
        "        self.film_left = FiLM(self.feature_dim)\n",
        "        self.film_right = FiLM(self.feature_dim)\n",
        "\n",
        "    def forward(self, x_left: torch.Tensor, x_right: torch.Tensor):\n",
        "        left_tiles = self._tile_image(x_left)\n",
        "        right_tiles = self._tile_image(x_right)\n",
        "        b, t, c, h, w = left_tiles.shape\n",
        "        left_tiles = left_tiles.view(b * t, c, h, w)\n",
        "        right_tiles = right_tiles.view(b * t, c, h, w)\n",
        "        f_left = self._encode_backbone(left_tiles).view(b, t, -1)\n",
        "        f_right = self._encode_backbone(right_tiles).view(b, t, -1)\n",
        "        f_left = self.film_left(f_left)\n",
        "        f_right = self.film_right(f_right)\n",
        "        f_left_mean = f_left.mean(dim=1)\n",
        "        f_right_mean = f_right.mean(dim=1)\n",
        "        fused = torch.cat([f_left_mean, f_right_mean], dim=1)\n",
        "        total = self.softplus(self.head_total(fused))\n",
        "        gdm = self.softplus(self.head_gdm(fused))\n",
        "        green = self.softplus(self.head_green(fused))\n",
        "        return total, gdm, green\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Weight loading & auto-detection ------------------------------------\n",
        "\n",
        "def _clean_state_dict(state: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
        "    if \"state_dict\" in state:\n",
        "        state = state[\"state_dict\"]\n",
        "    if \"model\" in state and isinstance(state[\"model\"], dict):\n",
        "        state = state[\"model\"]\n",
        "\n",
        "    new_state = {}\n",
        "    for k, v in state.items():\n",
        "        if k.startswith(\"module.\"):\n",
        "            k = k[len(\"module.\"):]\n",
        "        if k.startswith(\"student.\"):\n",
        "            k = k[len(\"student.\"):]\n",
        "        if any(skip in k for skip in [\"txt_enc\", \"text_encoder\", \"text_model\", \"img_proj\", \"txt_proj\", \"clip\", \"language\"]):\n",
        "            continue\n",
        "        new_state[k] = v\n",
        "    return new_state\n",
        "\n",
        "\n",
        "def _detect_variant(clean_keys: List[str]) -> str:\n",
        "    if any(\"film_left\" in k or \"film_right\" in k for k in clean_keys):\n",
        "        return \"tiled_film\"\n",
        "    if any(\"_tile\" in k or \"grid\" in k for k in clean_keys):\n",
        "        return \"tiled\"\n",
        "    return \"plain\"\n",
        "\n",
        "\n",
        "def _attempt_load(model: nn.Module, state: Dict[str, torch.Tensor]) -> bool:\n",
        "    missing, unexpected = model.load_state_dict(state, strict=False)\n",
        "    return len(missing) < len(state)\n",
        "\n",
        "\n",
        "def load_fold_model_auto(path: str, grid: Tuple[int, int] = GRID):\n",
        "    candidates = [\n",
        "        \"vit_large_patch14_dinov2.lvd142m\",\n",
        "        \"vit_base_patch14_dinov2.lvd142m\",\n",
        "        \"vit_small_patch14_dinov2.lvd142m\",\n",
        "        \"convnextv2_base.fcmae_ft_in22k_in1k\",\n",
        "    ]\n",
        "\n",
        "    try:\n",
        "        ckpt = torch.load(path, map_location=DEVICE, weights_only=True)\n",
        "    except TypeError:\n",
        "        ckpt = torch.load(path, map_location=DEVICE)\n",
        "    clean_state = _clean_state_dict(ckpt)\n",
        "    keys = list(clean_state.keys())\n",
        "    variant = _detect_variant(keys)\n",
        "\n",
        "    for backbone_name in candidates:\n",
        "        input_res = _infer_input_res(backbone_name)\n",
        "        if variant == \"plain\":\n",
        "            model = TwoStreamDINOPlain(backbone_name, input_res)\n",
        "        elif variant == \"tiled_film\":\n",
        "            model = TwoStreamDINOTiledFiLM(backbone_name, input_res, grid)\n",
        "        else:\n",
        "            model = TwoStreamDINOTiled(backbone_name, input_res, grid)\n",
        "\n",
        "        model.to(DEVICE)\n",
        "        ok = _attempt_load(model, clean_state)\n",
        "        if ok:\n",
        "            model.eval()\n",
        "            return model, variant, backbone_name, input_res\n",
        "    raise RuntimeError(f\"Failed to load checkpoint {path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TTA transforms ------------------------------------------------------\n",
        "\n",
        "def get_tta_transforms(img_size: int):\n",
        "    base = [\n",
        "        A.Resize(img_size, img_size),\n",
        "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "        ToTensorV2(),\n",
        "    ]\n",
        "    tta = []\n",
        "    tta.append(A.Compose(base))\n",
        "    tta.append(A.Compose([A.HorizontalFlip(p=1.0)] + base))\n",
        "    tta.append(A.Compose([A.VerticalFlip(p=1.0)] + base))\n",
        "    return tta\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inference helpers ---------------------------------------------------\n",
        "@torch.no_grad()\n",
        "def predict_one_view(models: List[nn.Module], loader: DataLoader):\n",
        "    all_preds = []\n",
        "    for batch in tqdm(loader, desc=\"Predict\", leave=False):\n",
        "        left, right = batch\n",
        "        left = left.to(DEVICE)\n",
        "        right = right.to(DEVICE)\n",
        "        model_preds = []\n",
        "        for model in models:\n",
        "            total, gdm, green = model(left, right)\n",
        "            dead = total - gdm\n",
        "            clover = gdm - green\n",
        "            stack = torch.cat([green, dead, clover, gdm, total], dim=1)\n",
        "            model_preds.append(stack)\n",
        "        stacked = torch.stack(model_preds, dim=0).mean(dim=0)\n",
        "        stacked = torch.clamp(stacked, min=0.0)\n",
        "        all_preds.append(stacked.cpu())\n",
        "    return torch.cat(all_preds, dim=0).numpy()\n",
        "\n",
        "\n",
        "def run_ensemble_prediction(ckpt_paths: List[str], weights: List[float], grid: Tuple[int, int], test_df: pd.DataFrame):\n",
        "    models = []\n",
        "    input_res = None\n",
        "    for path in ckpt_paths:\n",
        "        model, variant, backbone, input_res = load_fold_model_auto(path, grid)\n",
        "        print(f\"Loaded {path} as {variant} with {backbone} @ {input_res}\")\n",
        "        models.append(model)\n",
        "\n",
        "    assert input_res is not None\n",
        "    transforms = get_tta_transforms(input_res)\n",
        "    tta_preds = []\n",
        "    for transform in transforms:\n",
        "        dataset = TestBiomassDataset(test_df, TEST_IMAGE_DIR, transform, input_res)\n",
        "        loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
        "        preds = predict_one_view(models, loader)\n",
        "        tta_preds.append(preds)\n",
        "    tta_stack = np.stack(tta_preds, axis=0).mean(axis=0)\n",
        "\n",
        "    weights = np.array(weights, dtype=np.float32)\n",
        "    weights = weights / weights.sum()\n",
        "    weighted_models = tta_stack if len(models) == 1 else None\n",
        "    if weighted_models is None:\n",
        "        # Re-run per-model to apply weights\n",
        "        per_model_preds = []\n",
        "        for model in models:\n",
        "            per_tta = []\n",
        "            for transform in transforms:\n",
        "                dataset = TestBiomassDataset(test_df, TEST_IMAGE_DIR, transform, input_res)\n",
        "                loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
        "                preds = predict_one_view([model], loader)\n",
        "                per_tta.append(preds)\n",
        "            per_model_preds.append(np.stack(per_tta).mean(axis=0))\n",
        "        weighted_models = np.average(np.stack(per_model_preds, axis=0), axis=0, weights=weights)\n",
        "    return weighted_models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Submission creation -------------------------------------------------\n",
        "\n",
        "def create_submission(final_5: np.ndarray, test_long: pd.DataFrame, test_unique: pd.DataFrame, submission_path: str):\n",
        "    green = final_5[:, 0]\n",
        "    dead = final_5[:, 1]\n",
        "    clover = final_5[:, 2]\n",
        "    gdm = final_5[:, 3]\n",
        "    total = final_5[:, 4]\n",
        "\n",
        "    def nnz(x):\n",
        "        x = np.nan_to_num(x, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "        return np.maximum(0.0, x)\n",
        "\n",
        "    green, dead, clover, gdm, total = map(nnz, [green, dead, clover, gdm, total])\n",
        "\n",
        "    wide = pd.DataFrame({\n",
        "        \"image_path\": test_unique[\"image_path\"],\n",
        "        \"Dry_Green_g\": green,\n",
        "        \"Dry_Dead_g\": dead,\n",
        "        \"Dry_Clover_g\": clover,\n",
        "        \"GDM_g\": gdm,\n",
        "        \"Dry_Total_g\": total,\n",
        "    })\n",
        "\n",
        "    long_preds = wide.melt(\n",
        "        id_vars=[\"image_path\"],\n",
        "        value_vars=[\"Dry_Green_g\", \"Dry_Dead_g\", \"Dry_Clover_g\", \"GDM_g\", \"Dry_Total_g\"],\n",
        "        var_name=\"target_name\",\n",
        "        value_name=\"target\",\n",
        "    )\n",
        "\n",
        "    sub = pd.merge(\n",
        "        test_long[[\"sample_id\", \"image_path\", \"target_name\"]],\n",
        "        long_preds,\n",
        "        on=[\"image_path\", \"target_name\"],\n",
        "        how=\"left\",\n",
        "    )[[\"sample_id\", \"target\"]]\n",
        "\n",
        "    sub[\"target\"] = np.nan_to_num(sub[\"target\"], nan=0.0, posinf=0.0, neginf=0.0)\n",
        "    sub.to_csv(submission_path, index=False)\n",
        "    print(f\"Saved submission to: {submission_path}\")\n",
        "    display(sub.head())\n",
        "    return sub\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Main execution -----------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    test_long = pd.read_csv(TEST_CSV_PATH)\n",
        "    test_unique = test_long.drop_duplicates(subset=[\"image_path\"]).reset_index(drop=True)\n",
        "\n",
        "    ensembles = ENSEMBLE_GROUPS if ENSEMBLE_GROUPS else [\n",
        "        {\"name\": \"default\", \"weights\": [1.0] * len(CHECKPOINTS), \"ckpts\": CHECKPOINTS}\n",
        "    ]\n",
        "\n",
        "    final_preds = None\n",
        "    total_weight = 0.0\n",
        "    for group in ensembles:\n",
        "        ckpts = group.get(\"ckpts\", [])\n",
        "        if not ckpts:\n",
        "            ckpts = CHECKPOINTS\n",
        "        if not ckpts:\n",
        "            raise ValueError(\"No checkpoints provided. Please update CHECKPOINTS or ENSEMBLE_GROUPS.\")\n",
        "        weights = group.get(\"weights\", [1.0] * len(ckpts))\n",
        "        group_pred = run_ensemble_prediction(ckpts, weights, GRID, test_unique)\n",
        "        w = group.get(\"weight\", 1.0)\n",
        "        if final_preds is None:\n",
        "            final_preds = group_pred * w\n",
        "        else:\n",
        "            final_preds += group_pred * w\n",
        "        total_weight += w\n",
        "\n",
        "    final_preds = final_preds / max(total_weight, 1e-6)\n",
        "    _ = create_submission(final_preds, test_long, test_unique, SUBMISSION_FILE)\n",
        "\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}